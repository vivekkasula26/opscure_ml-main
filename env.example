# ===========================================
# OPSCURE AI PIPELINE CONFIGURATION
# Copy to .env and fill in your values
# ===========================================

# Ollama (Local LLM)
OLLAMA_URL=http://localhost:11434
# Override model for local testing if llama3 is missing
OLLAMA_MODEL=tinyllama
# Optional: Only if your Ollama/Remote server requires authentication
OLLAMA_API_KEY=

# Pinecone (Vector Store for RAG)
# Get API key from: https://www.pinecone.io/
PINECONE_API_KEY=your-pinecone-api-key
PINECONE_ENVIRONMENT=us-east-1
PINECONE_INDEX_NAME=opscure-incidents

# OpenAI (For embeddings)
# Get API key from: https://platform.openai.com/
OPENAI_API_KEY=your-openai-api-key
